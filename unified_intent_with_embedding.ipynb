
# Unified Intent Detection with BERT Embedding-Augmented FLAN-T5

!pip install -q transformers datasets accelerate sentence-transformers

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
from datasets import Dataset
from sentence_transformers import SentenceTransformer
import pandas as pd
import torch

# 1. Sample Data
examples = [
    {"input": "Send 500 to Arun", "target": '{"intent": "money_transfer", "recipient": "Arun", "amount": 500}'},
    {"input": "Book a cab to airport", "target": '{"intent": "book_ride", "destination": "airport"}'},
    {"input": "Recharge my phone with 199", "target": '{"intent": "recharge", "amount": 199}'},
    {"input": "Pay electricity bill", "target": '{"intent": "bill_payment", "type": "electricity"}'},
    {"input": "Transfer 1000 to Neha", "target": '{"intent": "money_transfer", "recipient": "Neha", "amount": 1000}'},
]

# Convert to HF dataset
df = pd.DataFrame(examples)
dataset = Dataset.from_pandas(df)

# 2. Embedding Model (SBERT)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Add embeddings to dataset
def embed(example):
    example["embedding"] = embedder.encode(example["input"]).tolist()
    return example

dataset = dataset.map(embed)

# 3. Tokenizer and FLAN-T5 Model
model_checkpoint = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# 4. Tokenization with embedding-aware concatenation
def preprocess(example):
    embed_text = " ".join([str(round(x, 4)) for x in example["embedding"][:8]])  # use only 8 dims for token length
    aug_input = f"{embed_text} | {example['input']}"
    model_inputs = tokenizer(aug_input, max_length=128, truncation=True)
    labels = tokenizer(example['target'], max_length=64, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_ds = dataset.map(preprocess)

# 5. Training
training_args = Seq2SeqTrainingArguments(
    output_dir="./flan_augmented_model",
    per_device_train_batch_size=4,
    num_train_epochs=10,
    logging_dir="./logs",
    logging_steps=1,
    evaluation_strategy="no",
    save_strategy="no",
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# 6. Inference
def predict(text):
    embed = embedder.encode(text)
    embed_text = " ".join([str(round(x, 4)) for x in embed[:8]])
    aug_input = f"{embed_text} | {text}"
    inputs = tokenizer(aug_input, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_length=64)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(predict("Send 200 to Priya"))
print(predict("Recharge Jio with 299"))
